\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{url}

\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\urlstyle{rm}
%
\begin{document}
%
\title{TBD}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Princeton University, Princeton NJ 08544, USA \and
Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
\email{lncs@springer.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
%The abstract should briefly summarize the contents of the paper in
%150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%
\section{Introduction and state of the art}

Concerns about the energy costs of computing infrastructure are not new \cite{sinha2001jouletrack,kurp2008green,lewis2008run}, but its importance and urgency has certainly increased in the last years with the introduction of metaheuristic and hyperheuristic algorithms \cite{irace,novoa2021measuring} as well as deep learning and large language models \cite{10549890,rodriguez2024evaluatingenergyconsumptionmachine}. With it, the concern on {\em greening} algorithms implementations has also grown. This energy optimization involves essentially designing a experiment framework where different configurations of the algorithm are compared with each other, by measuring the energy consumed by every one of them in a certain environment and applying statistical techniques to determine, for the workload under study, which configuration consumes the least energy.

These two stages of measuring and applying statistical techniques to compare are critical. However, they face several challenges. The first one is that unlike other performance metrics, it is impossible to measure what one specific process running in an operating system is consuming. Since energy sensors and counters operate system-wide, the best option you have is to take measurements of different subsystems or the whole system synchronized with the start and end of the process; this implies that, after the experiment, you need to separate what is the system overhead from the workload you are measuring. Baseline measurements will be taken to include the system overhead as well as the overhead of the framework the algorithm is implemented in; workload measurements will then usually subtract those baseline measurements to get to the {\em real} amount of energy consumed.

The second challenge is even more determinant: the operational context of the software and hardware system, which include processor states as well as operating conditions such as the temperature will influence the energy measurements, and it will do so not only in a way that affects the baseline measurements, but also the relative differences between the baseline and workload+baseline \cite{freina2024survey,cruz2025}. There are several reasons for this happening but they can be summarized in the fact that the chipset and the operating system will actively work towards keeping the temperature of the system within a certain range and certainly avoid it going over a critical threshold. Additionally to this task, over which the user has little or no control, modern system actively manage power consumption looking for a minimization using different techniques such as dynamic voltage and frequency scaling (DVFS) or dynamic power management (DPM) \cite{snowdon2005power}.

% No me queda claro (al principio) que es 'state of the system'.
% Ya al hablar de chipset + software me queda claro que es sistema completo.
% Podemos agregar al interprete también.
% Se podría utilizar algo más intuitivo como 'system dinamics' o 'system operational context'
% https://en.wikipedia.org/wiki/Operating_context
% state = operational context
% JJ - lo he cambiado un poco

The fact that, in general, scientists will work on personal computers, implies that the measure of energy we are interested in is the one taken in a real system, where other processes will be running at the same time. In general, this means that the state of the system will be changing all the time: measures for temperature as well as power reduction will be active the whole duration of any experiment, but the most important thing is that they will be triggered not only by the workload, even if it is a substantial one, one that implicates several cores, but also by others; that is, there is no control over the {\em base} state of the system, however you want to define it, and no control over how these measures will affect the workload that is being measured.

One possible answer to this is to try to control the state as much as possible, eliminating all possible interference. However, projecting these measurements to {\em real} systems might be tricky, since the fact that system state and workload interact in unknown ways might mean that a specific configuration or parametrization works better in a specific state might be the worse when it interacts with the system in a different state.

Despite this known influence of system state on energy measurements, empiric research of how this influence works and how it might affect comparison of configurations is relatively scarce; a very interesting paper, \cite{cotta24}, however, drew researchers attention to the fact that system temperature reached under one specific run of the experiment will influence the next one, and proposed measures to mitigate it. Later, \cite{cotta25} proved that these changes in temperature and other system state will affect the next measurements in a sequence, having a kind of {\em hysteretic} effect where the system does not fall back to the previous, pre-experiment state, but remains in a state with higher temperature that affects measurements. However, the relationship between power (or energy) measurements had already been established: \cite{devogeleer2014modelingtemperaturebiaspower} mentions a quadratic relationship in the range 20º to 50º, although it might be exponential for temperatures higher that that and up to 80º. This is beyond the critical threshold of most systems nowadays, so the quadratic model is probably closer to current reality. A more modern experimental analysis \cite{PENG2025104837} mention also the exponential relationship between power consumption and temperature, introducing a new core layout algorithm that reduces power consumption. On one hand, it is a proof that energy consumption concerns are being tackled at all levels, including processor design; on the other, it is quite clear that this cannot be leveraged by a scientist to lower energy consumption in the algorithms they are working with and their implementations.

This calls, anyway, for more extensive experimentation on the general relationship between temperature and energy measurements, and to what extent energy measurements are a result of the temperature of the CPU package, as opposed to the parametrization or implementation of the workload itself.
This is precisely what we will carry out in this paper. We will run a set of experiments and analyze them looking for a model of influence of the CPU temperature, % system's temperature ? Changed to CPU temperature - JJ
as captured by sensors, in the energy spent by a workload. Once that has been established, we will work with the system design and its operating context to try and carry out comparison of different configurations attempting to determine which one is more energy efficient and to what extent we can utilize knowledge of system temperature to reduce the amount of energy spent by our metaheuristic experiments.

% Algunos puntos:
% El efecto de la temperatura es distinta para cada máquina específica y su software.
% Siempre se debe generar el modelo antes de correr experimentos? (incluso en la misma máquina).
% Aún durante la ejecución de los experimentos podría ser necesario acutalizar el modelo?.
%
% Nota: Tal vez podría considerarse un modelo con reglas difusas que pudiera ser capaz
% de ser más general.
% JJ- estamos usando modelos estadísticos, por lo pronto. La idea es tratar de separar cuanta energía se consume por la temperatura y cuanta por la implementación real del algoritmo y las diferentes parametrizaciones. No sé si un modelo de reglas difusas ayudará, pero estoy abierto a sugerencias.

The rest of the paper is organized as follows: next we will describe the experimental design, followed by a Section devoted to the results of the experiments (Section \ref{sec:exp}). Finally, we will discuss these results and draw our conclusions.

\section{Experiments and results}\label{sec:exp}

Our interest in heuristic measurement of the influence of temperature in energy consumption arose from out line of research that tries to optimize it in metaheuristics; this is why we will be making measurements on a population-based metaheuristic called Brave New algorithm \cite{bna}. The implementation has been made on the language Julia, a JIT-compiled language with an interesting concurrent model. However, we will be using a single-threaded process and testing different configurations for minimizing the Sphere function using different configurations of the implementation: problem dimension (3, 5 or 10) and population size (200 or 400). Our initial intention was to compare different parameter, software and hardware configurations; however, for the purpose of this paper, the main points is that we will be running experiments every one of which takes several seconds, with the whole experiment for all configurations taking about two hours. The duration is enough to make the system enter different states, and so that it theoretically have enough energy consumption differences between configuration to be detectable.

Besides these two parameters, we will also be changing another algorithm-specific parameter that is irrelevant for the description of the measurements. The experiments proceed in a sequence we have called {\em sandwich}: for every combination of three parameters, we start and end with a baseline  experiment (which simply generates the population with the dimension and population size set), and runs in the middle a workload experiment. For every 3-parameter configuration we will run 30 repetitions for the workload, and 31 for the baseline. The point of running experiments this way has been justified elsewhere, and mainly has the objective of mitigating the variability of the difference in energy consumption of the baseline and workload experiments by subtracting the average of the two baseline measurements of the workload one.

These whole set of experiments is then repeated several times at several points after booting the system, doing normal work in between. We repeat 4 times every experiment this way, the main idea being to {\em catch} the system in different states during the experiment, so that we can have a more precise model of their dependence.

All measures have been taken on the same system, a desktop system with an AMD Ryzen 9 9950X 16-Core Processor. This is a two-die processor, that has the cores distributed in two different dices, each one with its own temperature sensor. We will take into account this fact in the analysis, as well as the experimental design. This machine is running Ubuntu with kernel {\tt 6.17.0-14-generic}. We are using Julia version 1.11.9, initially with the test version and then the stable one, although in this paper we will not be comparing hardware versions. We are using {\sf pinpoint}, a command line tool that reads energy measurements from the RAPL API \cite{rapl,khan2018rapl}. Since this is an AMD system, only PKG (package) energy can be measured, without distinguishing between different processor planes or memory spent by DRAM. This is enough, however, to understand the interdependence between temperature and energy spent by our implementation of the algorithm. Finally, the {\sf sensors} command is used to read the temperature from the sensors; a specific sensor, {\sf k10temp-pci-00c3}, yields three measurements, one for maximum allowed temperature and one for each die: {\sf Tccd1} and {\sf Tccd2}. Every single run, in superuser mode, is executed from a Perl script, that also processes the energy and temperature readings and outputs a CSV file. All data files are available in a public GitHub repository \url{https://github.com/CeciMerelo/BraveNewAlgorithm.jl}.

We will run the initial experiment, using the same conditions; this is intended as a baseline.
%
<<europar.initial, echo=FALSE, fig.cap="Energy spent vs. temperature for the baseline experiment. Different colors indicate the problem dimension.", fig.height=4>>=
load("data/europar_test.rds")
library(ggplot2)
europar_test$dimension <- as.factor(europar_test$dimension)
europar_test$base <- ifelse( startsWith(europar_test$work, "base"), TRUE, FALSE)
ggplot(europar_test, aes(x=initial_temp, y=PKG, color=base, shape=dimension)) + geom_point()  + labs( title = "Energy Consumption Over Temperature", x = "Temperature", y = "Energy Consumption " ) + theme_minimal()
@

In Figure \ref{fig:europar.initial} we plot the PKG energy consumed against the initial temperature. Different dimensions translate to different shapes, and color indicate whether it is a baseline run (without workload, just generation of the initial population) or workload run. We can observe a certain dependence between temperature and energy consumed, which grow in a different way for runs with or without workload. We will then need to consider different models for them.

<<europar.initial.base.model, echo=FALSE, message=FALSE,warning=FALSE, fig.cap="Model of energy consumption for the baseline experiment.", fig.height=4>>=
library(dplyr)
europar_test_base <- europar_test %>% filter(base == TRUE)
europar_test_base_model_linear <- glm( PKG ~ + initial_temp*dimension,data = europar_test_base)
europar_test_base_model <- glm( PKG ~ I(initial_temp^2) + initial_temp*dimension*population_size,data = europar_test_base)
difference_base_models <- anova(europar_test_base_model,europar_test_base_model_linear)
@

We will try to adjust two different generalized linear models to this data (using R's {\sf glm}), one linear on the initial temperature and other that includes also a quadratic term; in both cases we will also include the problem dimension (a problem parameter) and the population size (an algorithm parameter). Using ANOVA to evaluate the difference between the models, the quadratic model is significantly better with a residual deviation of \Sexpr{round(difference_base_models$`Resid. Dev`[1],2)} compared to \Sexpr{round(difference_base_models$`Resid. Dev`[2],2)} for the linear model. This model has a positive coefficient for the initial temperature, but a negative for the quadratic term of a much lower magnitude, indicating that the dependence of energy consumption with temperature increases more slowly as it rises, probably due to throttling measures by the system.

In principle, this variability was expected which is why every workload run is between two baseline runs, expected to eliminate at least part of the variability from the measurement.

<<europar.initial.workload.model, echo=FALSE, message=FALSE,warning=FALSE, fig.cap="Model of energy consumption for the workload experiment.", fig.height=4>>=
source("R/process_deltas.R")
europar_test_processed <- process_deltas( europar_test )
europar_test_processed$dimension <- as.factor(europar_test_processed$dimension)
europar_test_processed$population_size <- as.factor(europar_test_processed$population_size)
ggplot(europar_test_processed, aes(x = initial_temp, y = delta_PKG)) +
  geom_point(color=europar_test_processed$dimension ) +
  labs(
    title = "Energy Consumption vs. Temperature",
    x = "Temperature",
    y = "Delta PKG"
  ) + theme_minimal()
europar_test_delta_model_linear <- glm( PKG ~ + initial_temp*dimension*population_size,data = europar_test_processed)
europar_test_delta_model <- glm( PKG ~ I(initial_temp^2) + initial_temp*dimension*population_size,data = europar_test_processed)
europar_test_delta_evals_model <- glm( PKG ~ I(initial_temp^2) + initial_temp*dimension*population_size+evaluations,data = europar_test_processed)
difference_delta_models <- anova(europar_test_delta_evals_model,europar_test_delta_model)
@

Figure \ref{fig:europar.initial.workload.model} shows the difference in energy consumption, Delta PKG, vs. temperature, with different colors for different problem sizes. In some cases this delta can be negative if the conditions from the baseline to the workload runs have changed dramatically. The variability is still quite high, with deltas reaching 400 Joules, and increment slightly with temperature. Again, we try to model this dependence using a generalized linear model, using a term quadratic on the temperature or not, and including population size, the dimension as factor and the number of evaluations, which should change every run as a result of the stochastic algorithm. Again, model quadratic in the temperature that includes the number of evaluations is better with \Sexpr{round(difference_delta_models$`Resid. Dev`[1],2)} compared to \Sexpr{round(difference_delta_models$`Resid. Dev`[2],2)}.
The model includes significant dependence on the initial temperature, population size the problem dimension, with evaluations being also significant. Dimension 10 (which is run at the beginning of every experiment) does not have any significant influence either by itself or in combination with any other. Again, the value of the quadratic term \Sexpr{round(coef(europar_test_delta_model)["I(initial_temp^2)"],4)} is negative and of a much lower magnitude than the linear term \Sexpr{round(coef(europar_test_delta_model)["initial_temp"],4)}.

<<europar.initial.model.slopes, echo=FALSE, message=FALSE,warning=FALSE, fig.cap="Slopes of the model for the workload experiment for the population sizes experimented.", fig.height=3>>=
library(marginaleffects)
plot_predictions(europar_test_delta_evals_model, condition = c("initial_temp","population_size", "dimension"))
@

This model is plotted in Figure \ref{fig:europar.initial.model.slopes} for the three problem dimensions and the algorithm parameter, population size, should allow us to draw some conclusions on which one should be chosen to minimize energy; however, given that the curves for the different populations are different depending on the dimension, cross each other at relatively plausible temperatures (35º in the case of dimensions 3 and 5) and do not clearly dominate each other for all dimensions, the only plausible conclusion is that the dependence of energy consumption on temperature should be modeled, and then a decision taken on the parameter values based on the most probable temperature of the system at the time of running the algorithm.

<<europar.base.time, echo=FALSE, message=FALSE,warning=FALSE, fig.cap="Time spent by the baseline experiment vs. temperature.", fig.height=4>>=
europar_test_base_model_time <- glm( PKG ~ I(initial_temp^2) + initial_temp*dimension*population_size+seconds,data = europar_test_base)
anova_time <- anova(europar_test_base_model_time,europar_test_base_model)
@

The operational context of the system will be reflected on other conditions besides the temperature. In general, what the operating system scheduler will do is to try and assign processes to cores inside the processor based on their predicted performance. This will depend on several factors, but two are the most important: the core load and the frequency at which the core is running. Many modern processors, and among them the Ryzen, will have cores running at different, dynamic, frequencies; in the case of the Ryzen these are also divided by die, so that cores in die 1 are considered "performance" cores, while cores in die 2 are "efficiency" cores, more energetically efficient.

Assigning a process to a lower-frequency core will certainly reduce energy consumption; however, we can only measure the amount of energy spent by the whole package. We will notice the use of lower-frequency cores by the time spent by our process. Time will, then, need to be incorporated into our model in order to take into account this effect. The new model is able to explain  more of the variability of the energy consumption, with a residual deviance of \Sexpr{round(anova_time$`Resid. Dev`[1],2)} compared to \Sexpr{round(anova_time$`Resid. Dev`[2],2)} for the model without time. Together, the two temperature independent terms and the running time are able to explain most of the energy variability, their importance being similar.

Since we cannot simply lower the temperature of the system, we need to try different approaches and measure how they influence this PKG vs. temperature model; in principle, we intend the following proposals as a measurement methodology, but it can later be applied to an actual methodology for energy optimization of metaheuristics.

\section{Discussion and conclusions}\label{sec:conclusions}

\begin{credits}
\subsubsection{\ackname}

his work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y Competitividad (Spanish Ministry of Competitivity and Economy) under project PID2023-147409NB-C21.

\end{credits}

\bibliographystyle{splncs04}
\bibliography{energy,ga-energy}
%

\end{document}

