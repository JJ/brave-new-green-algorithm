% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{color}

\begin{document}
%\SweaveOpts{concordance=TRUE}
%

<<setup, include=FALSE>>=
knitr::opts_chunk$set(concordance = TRUE)
@

\title{}
%
\titlerunning{}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{JJ Merelo\inst{1,2}\orcidID{ 0000-0002-1385-9741 } \and
%Cecilia Merelo-Molina\inst{3}}
\author{No Author\inst{1}}
%
%\authorrunning{Merelo and Merelo}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{No Institute}
%\institute{Department of Computer Engineering, Automatics and Robotics, University of Granada \email{jmerelo@ugr.es}\and
%CITIC, UGR, Spain
%\and
%Zenzorrito, Granada, Spain}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% Green computing tries to push a series of best practices that, in general, reduce the amount of energy consumed to perform a given piece of work. There are no fixed rules for {\em greening} an algorithm implementation, which means that we need to create a methodology that, after profiling the energy spent by an algorithm implementation, comes up with specific rules that will optimize the amount of energy spent. In population based algorithms, the exploration/exploitation balance is one of the most critical aspects. The algorithm we will be working with in this paper called Brave New Algorithm was designed with the main objective of keeping that balance in an optimal way through the stratification of the population. In this paper we will analyze how this balance affects the energy consumption of the algorithm.


\keywords{Green computing  \and Energy profiling \and Metaheuristics.}
\end{abstract}
%
%
%
\section{Introduction}

% We need to rewrite this

% Goldberg in his "Zen and the art of genetic algorithms" \cite{goldberg1989zen} included as a {\em koan} "Let nature be your guide". This has spawned all kind of population-based metaheuristics that use (and possibly abuse \cite{soni2021critical}) metaphors to inspire all kind of optimization algorithms, in most cases population-based.
%
% All these algorithms have characteristics in common. Generally are population-based, that is, they work with a population of individuals that are (possibly encoded) solutions to a problem. There possible solutions are evaluated via a so-called {\em fitness} or {\em objective} function, according to which population is modified so that it is increasingly more likely to find solutions that are closer to the objective. What the algorithms do is explore the search space, using a combination of techniques:\begin{itemize}
% \item {\em Exploration} techniques, which sample the search space looking for new solutions, generally via variation of the existing solutions.
% \item {\em Exploitation} techniques, which try to improve existing solutions by first selecting those that are closer to the objective, and then combining them to create new solutions, thus {\em exploiting} the search directions already present in the population.
% \end{itemize}
%
% The algorithms and their specific implementations need to keep exploration in check, so that only {\em promising} parts of the search space are checked, that is, only solutions that have a certain probability of being better are generated; leaning on exploration will eventually lead to an algorithm that is even worse than exhaustive search, since probabilistically some solutions can be generated several times. Exploitation will, on the other hand, focus on some specific areas of the search space, those that already have {\em good} solutions, potentially falling into local minima and abandoning other areas that could hold much better solutions. This is why there needs to be a balanced approach to using both techniques, as most algorithms already do.
%
% The performance of these algorithms is generally measured using quantities such as the number of fitness evaluations needed to reach a certain level of quality or the time needed to reach it. However, from the point of view of modern engineering energy needed by an algorithm implementation is as important, or even more, since an efficient implementation of an algorithm needs to be energy-efficient, or at least consume the least amount of energy possible.
%
% This will be our main intention in this paper. We will work with an algorithm proposed in \cite{merelo2022brave} called Brave New Algorithm, which, instead of being biologically inspired directly, gets its main population structures from the dystopic novel Brave New World by Aldous Huxley \cite{huxley2022brave}; hence the name. This algorithm was designed with the main objective of keeping a healthy exploration/exploitation balance, which besides can be easily tuned through algorithm parameters. This makes it a good candidate for the analysis we will perform on this paper.
%
% As secondary objective, we will be exploring the possibilities of implementation of metaheuristics in the language called Julia \cite{perkel2019julia}, a just-in-time compiled language that has an interesting way of dealing with allocation of memory; we will need to find out how, in this specific case, working with memory will affect consumption and how these mechanisms can be leveraged to optimize energy consumption.

The rest of the paper is organized as follows: next Section describes the state of the art in exploration of energy consumption of evolutionary algorithms and other metaheuristics. Section \ref{sec:bna} describes the algorithm and how it has been parametrized for the purpose of this paper. Since there is no fixed methodology for measuring energy consumption, we will show how it was done for this experiment and the tradeoffs it implies, together with the results, in Section \ref{sec:results}; these results will be discussed in the last Section \ref{sec:discussion} along with our conclusions.

\section{State of the art}

% This needs to be rewritten/improved.
As a secondary objective in this paper, we have wanted to deal with implementation issues in a non-mainstream language; as a matter of fact, there are several implementations of evolutionary algorithms in Julia, such as \cite{sanchez2023evolp}. Before that,  Julia had been used \cite{merelo2016comparison} as a baseline for comparing performance in basic evolutionary algorithms; as a matter of fact, we used Julia as a baseline, with its performance being more or less the median of all languages tested. Some other languages like Java or C++ might be several orders of magnitude faster. However, performance does not translate directly to energy consumption, and besides, back then Julia was still in its early versions vs. the more mature implementation that is now in production.

Moreover, our main interest is optimizing energy consumption. This can be done at many different levels, from the lowest, hardware level, to the implementation level, up to the algorithmic level, clearly at this level the range of possible optimizations will be relatively small, but still it gives leverage to scientists that cannot change any other level to make their algorithm implementation {\em greener}. Some researchers have applied it to machine learning algorithms \cite{gutierrez2022analysing}; there are not so many cases where it has been applied to metaheuristics such as the one used in this paper. One of these was \cite{diaz2022population}, where they work mainly on the population, and conclude that an increasing population, which implies increased diversity and possibly better overall performance, has a complex relationship with energy consumption that might not be a direct one. If we just equate energy consumption with time needed to reach a solution, finding a parametrization that finds solution faster will always be a way of reducing it.

In this paper we will be focusing on such a parametrization mainly, applied to the Brave New Algorithm, which is essentially an evolutionary algorithm with population reproduction restrictions. We will describe this algorithm next.

\section{A Brave New Algorithm}\label{sec:bna}

The Brave New Algorithm \cite{merelo2022brave-anon} is essentially an evolutionary algorithm, using the usual mechanism of mutation, crossover and selection to find the optimum of a given function, represented as a {\em chromosome}, in this case a vector of real numbers. However, how selection and reproduction takes place is similar to how the novel Brave New World \cite{huxley2022brave} organized society: in castes. \begin{itemize}
\item $\alpha$ generates new members by coupling the best individuals in its caste between them, to then undergo mutation and crossover.
\item $\beta$ caste needs to reproduce a member of of the $\alpha$ caste, and another member of its own caste, to then undergo mutation and crossover.
\item $\delta$ and $\epsilon$ castes generate new individuals by mutation. The only difference between them is that those who reproduce are chosen among the other members of the population, and that mutation rates can be set separately.
\item $\gamma$ generates new members by mutation, but mutation might be applied several times if the new individual is better than the previous one.


There are many possible choices for mutation and crossover operators. In this phase we will not focus so much in efficiency as in ease of implementation. This is what we will use for operators:\begin{itemize}
\item Mutation will be governed by a probability that indicates whether one specific component of the vector is going to be changed or not; when it changes, a new number is generated within the original range\footnote{Please note that we are not using the usual Gaussian Mutation here. It can be argued that a simple random mutation will have more or less the same effect; however, from our point of view, we are mainly interested in an operator that has a certain energy consumption and that performs exploration}.
\item Crossover is a many-point crossover, with several elements of the chromosomes selected randomly and interchanged between them.
\item Selection uses binary tournament \cite{blickle1996comparison}, also a very simple method that involves only a comparison.
\end{itemize}

These specific operator versions of the operators are not essential to the algorithm, and can be changed at will; however, these are the ones used in this paper.

\end{itemize}

After every generation, the population is re-ranked again, with the first $\alpha$ percentage assigned to that caste, and so on; through generations every caste will hold the same number of individuals. Since the two {\em upper} castes are mainly devoted to exploitation and the rest is devoted to exploration, the proportion of individuals in them is the main parameter governing the balance.
There are some restrictions in these percentages, due to the way new members are generated: the beta caste needs to have twice as many members as the alpha caste; the percentage of population in the $\alpha$ caste is, thus, the main parameter for this, since we can use it to generate the rest. In practice, what we have done is to vary the percentage of the $\gamma$ caste, the "first" that performs local search, while keeping $\delta$ and $\epsilon$ fixed to a low value.

The implementation of the algorithm used is available under a free license at \url{hidden.url}.
%\url{https://github.com/cecimerelo/BraveNewAlgorithm.jl}

\section{Experimental methodology and results}\label{sec:results}

In order to carry out these experiments, we have used a methodology that is similar to the one used in our previous papers: \cite{low-level-anon}. We employ {\sf pinpoint}, \cite{pinpoint}, a command line tool that taps the RAPL interface \cite{rapl} to measure energy consumption of a given process. The version we have used was compiled from commit {\tt dfee658}. We carry experiments in an AMD Ryzen 9 9950X 16-Core Processor, with Ubuntu Linux 25.04 with kernel version 6.14.0-29-generic. Julia version has been $1.11.7$.\footnote{Julia has now changed the minor version to 1.12, with $1.12.1$ being the latest released at the moment of writing this paper. However, there are some performance issues with this new version, which has prevented us for using it for the time being.}

We have used the Sphere function \cite{hansen2010comparing}, defined as sum of the squares of the distance to the center minus a fixed value, which is part of the BBOB benchmarks; as a matter of fact, the implementation of this function is taken from the {\sf BlackBoxOptimizationBenchmarking.jl}, also written in Julia. This function is also lightweight enough to allow us to compare the energy profiles of different combinations of genetic operators, which will then have a bigger impact on the overall consumption.

Our initial experimental design includes two problem sizes: vectors with 3 or 5 dimensions, to check the impact of the parameters for different problem difficulties. This corresponds to the low end of the BBOB benchmark suites; remember that we are using this suit just for the purpose of profiling the energy for this language and algorithm, so for the time being we did not find it necessary to go into higher dimensions.

\begin{itemize}
\item Population size: 200 or 400 individuals, which has an impact in the diversity of the population and thus the exploitation capabilities of the algorithm.
\item Maximum number of generations without improvement: 10 or 50, which is the stopping criterion for the algorithm. In this paper we have made this specific choice for stopping criterion, because if the algorithm tends towards exploration, it will explore in fruitless directions generating (possibly) useless chromosomes that will not allow it to escape a local minimum; in particular, this will have the consequence that depending on the problem dimension the execution might fall well short of an optimum value. We have left it this way, however, due to its direct relationship with exploration.
\item Percentage of population in the $\alpha$ caste: 10\% or 25\%, which is the main parameter that controls the exploration/exploitation balance, since only the $\alpha$ and $\beta$ castes perform exploitation. The rest of the parameters will be set accordingly: $\beta$ percentage will be double that value. $\gamma$ caste will hold a proportion equal to the $\alpha$ and $\beta$ percentages subtracted from 90.
$\delta$ and $\epsilon$ castes will always have the same proportion, each equal to 5\%.
\end{itemize}

\begin{table}[h!tb]
\centering
\caption{Caste percentages depending on $\alpha$ proportion used.}\label{tab:params}
\begin{tabular}{lcc}
\toprule
$\alpha$ percentage & Exploration proportion & Exploitation proportion \\
\midrule
10\% & 30\% & 70\% \\
25\% & 75\% & 25\% \\
\end{tabular}
\end{table}

This means that the $\alpha$ percentage is the main one governing this exploration/exploitation balance. Please check table \ref{tab:params} for how the explorative/exploitative castes are organized depeding on the values used.
Every parametrization runs 30 times sequentially in a single core. Results written in standard output are processed, and are available from this paper's repository at \url{hidden.url}.
%\url{https://github.com/JJ/brave-new-green-algorithm}.


Until version 1.11, Julia does not produce executables. Julia loads the script every time it is run, uses just-in-time compilation to generate low-level code, and this code is then run within its own virtual machine. This compilation, which might include recompilation of modules, takes a certain amount of time and consumes a certain amount of energy, adding to the system overhead. This is why in this case it is specially important to make baseline measurements to discount these two quantities when we make measurements on the actual workload.

The main issue is {\em what} to consider baseline. One well-established principle is that you need to use exactly the same code for it and the workload, so a common option is to use a command line flag to indicate, in runtime, what kind of workload is being run. The second issue is what to run as such baseline workload. Since we are dealing with population-based algorithms, a straightforward option is to use generation of the population as such baseline workload. This is what we have done in this case: a flag indicates whether the program should stop after the initial generation or continue to perform the algorithm. This in turn means that there will be different baselines depending on the chromosome dimension and population size, four in total. For every parametrization, we will run 240 experiments; after an initial run with 120, we performed another one to increment the statistical accuracy.

<<evoapps.baseline.results, echo=FALSE, warnings=FALSE, message=FALSE, fig.height=3, fig.pos="h!tb", fig.cap="PKG energy (in Joules) as a function of time for baseline experiments; shapes are related to population size and color to problem dimension">>=
library(dplyr)
# read RDS file data/energy-full-data.rds
readRDS("data/energy-full-data.rds") -> baseline_results
baseline_results$work <- "BNA-baseline"

baseline_results %>% group_by(dimension,population_size) %>%
  summarise(median_energy=median(PKG), sd_energy=sd(PKG),
            trimmed_mean_energy=mean(PKG,trim=0.2),
            median_time=median(seconds), sd_time=sd(seconds),
            trimmed_mean_time=mean(seconds, trim=0.2)) -> summary_baseline_results

# plot energy vs. evaluations with linear model walcom_lm
library(ggplot2)
baseline_results$shape <- ifelse(baseline_results$population_size==200,21,24)
baseline_results$color <- ifelse(baseline_results$dimension==3,"red","blue")
ggplot(baseline_results, aes(x=seconds,y=PKG)) + geom_point( shape=baseline_results$shape, color=baseline_results$color) + geom_smooth(method="lm", formula=y~x, se=FALSE)
@

Figure \ref{fig:evoapps.baseline.results} plots the PKG energy (in Joules) vs. time taken for every experiment, with triangles representing a population of size 400 and circles for size 200; blue is for dimension = 5 and red for dimension = 3. In general, energy consumption will depend linearly on time, but there are clusters of data points with the same shape and color that hint at these having an influence on energy consumption beyond simple time. We should also observe the dispersion in a range of several hundred milliseconds as well as several hundreds of Joules, although most results are below 350 Joules and 6.8 seconds. This dispersion, however, is a real issue. Besides hysteretic effects that have repeatedly been observed in this context \cite{10.1145/3638530.3664093}, there is an additional issue that occurs when measurements take a relatively long time: some system-wide issues might affect one of the parameter combinations more than others.

<<evoapps.baseline.violin, echo=FALSE, warnings=FALSE, message=FALSE, fig.height=3, fig.pos="h!tb", fig.cap="Violin plot for the distribution of energy values for every parameter value">>=
ggplot(baseline_results, aes(x=factor(dimension),fill=factor(population_size), y=PKG)) + geom_violin() + theme_minimal() + labs(x="Dimension", y="Energy (J)") + scale_y_log10()
@

What we see in Figure \ref{fig:evoapps.baseline.violin} is a violin plot that shows a big dispersion, over all for the combination of dimension = 3 and population = 200; not only that, but there are three clusters along the "violin". These three accumulations for certain energy values happen all across the parameter values.

Any of these variables have a high variance and many outliers. This is why we need to carefully consider what kind of statistical measure we should use for summarizing the results so that they can be subtracted from the measurements that carry the workload. Finally we settled on the {\em trimmed mean}, eliminating the 20\% values with the highest and lowest values. This gives a more robust central tendency measure, which is also closer to the median.

We should take into account that, as stated elsewhere \cite{cpp.vs.zig-anon}, profiling for every kind of algorithm needs its own methodology that is adequate to the dispersion in the measurements taken; however, a high dispersion combined with time (or temperature) related effects will always lead to a certain amount of uncertainty in the conclusions. That will not invalidate these conclusions, but they will have to be explained and factored in when  any kind of result is reached.

The results obtained after performing measurements with the actual workload will be examined next. We will be running the full algorithm with the indicated termination condition for 30 runs. We will subtract the baseline trimmed average for PKG energy as well as time from every result. The PKG energy vs. time chart is shown in Figure \ref{fig:evoapps.results}.

<<evoapps.results, echo=FALSE, warnings=FALSE, message=FALSE, fig.height=3, fig.pos="h!tb", fig.cap="Energy spent by the workload as a function of time for all experiments; point size is related to the max number of generations without change; fill color is blue for dimension =5, red for dimension = 3; shapes are related to population size and finally point size to the number of generations without improvement.">>=
library(dplyr)
evoapps_results <- read.csv("data/sphere-evoapps-1.11.7-full-bna-16-Oct-17-32-01.csv")

evoapps_results$delta_PKG <- 0
evoapps_results$delta_seconds <- 0

for (dim in c(3,5)) {
  for ( pop_size in c(200,400)) {
    number_of_rows <- nrow(evoapps_results[ evoapps_results$dimension==dim & evoapps_results$population_size==pop_size,])
    evoapps_results[ evoapps_results$dimension==dim & evoapps_results$population_size==pop_size,]$delta_PKG <-
      evoapps_results[ evoapps_results$dimension==dim & evoapps_results$population_size==pop_size,]$PKG  -
      rep(summary_baseline_results[ summary_baseline_results$population_size == pop_size & summary_baseline_results$dimension==dim, ]$median_energy,number_of_rows)

    evoapps_results[ evoapps_results$dimension==dim & evoapps_results$population_size==pop_size,]$delta_seconds <-
      evoapps_results[ evoapps_results$dimension==dim & evoapps_results$population_size==pop_size,]$seconds  -
      rep(summary_baseline_results[ summary_baseline_results$population_size == pop_size & summary_baseline_results$dimension==dim, ]$median_time,number_of_rows)
  }
}

evoapps_results %>% group_by(dimension,population_size,max_gens,alpha) %>%
  summarise(mean_energy=mean(delta_PKG), sd_energy=sd(delta_PKG),
            mean_time=mean(delta_seconds,trim = 0.1), sd_time=sd(delta_seconds)) -> summary_evoapps_results

# plot energy vs. evaluations with linear model walcom_lm
library(ggplot2)
evoapps_results$population_size <- as.factor(evoapps_results$population_size)
evoapps_results$point_size <- ifelse(evoapps_results$max_gens==10,2,4)
evoapps_results$shape <- ifelse(evoapps_results$population_size==200,21,24)
evoapps_results$color <- ifelse(evoapps_results$dimension==3,"red","blue")
ggplot(evoapps_results, aes(x=delta_PKG,y=delta_seconds)) + geom_point(fill=evoapps_results$color,size=evoapps_results$point_size,shape=evoapps_results$shape) + geom_smooth(method="lm", formula=y~x, se=FALSE)+theme(legend.position="none") + labs(x="Energy (J)", y="Time (s)")
@

We see again that energy is roughly related to the time taken, but we can also draw more conclusions. In general, "triangles" with population size = 400 are {\em above} the line, that is, they consume less energy that would be usual for the time taken. If we take into account that the generation of the population is included in the baseline, this essentially means that most of the energy needed for bigger populations is consumed when that population is generated for the first time; population size does not need so much energy for the rest of the algorithm. Circles are below, so the opposite is true. What does seem to have certain importance is the dimension of the problem: dimension = 5 in blue are placed, in general, to the right hand side indicating they need more time and also more energy. Finally, the point size, which is related to the maximum number of generations without improvement, seems to be  related to energy consumption: more generations without improvement means more consumption.

We are more interested, however, in the exploration vs. exploitation balance, which is governed by the amount of individuals in the $\alpha$ caste. The results for different combinations are shown in Figure \ref{fig:evoapps.alpha}. The boxplot reveals that the difference in energy spent is not really significant, at least for this version of the algorithm, which has some issues.

<<evoapps.alpha, echo=FALSE, warnings=FALSE, message=FALSE, fig.height=3, fig.pos="h!tb", fig.cap="Boxplot of energy spent for different population/dimension combinations and the percentage of population in the alpha caste.">>=
evoapps_results$alpha <- as.factor(evoapps_results$alpha)
evoapps_results$dim_pop <- paste0("D:",evoapps_results$dimension," Pop:",evoapps_results$population_size)
ggplot(evoapps_results, aes(x=dim_pop, y=delta_PKG,fill=factor(alpha))) + geom_boxplot( notch=T, position="dodge") + theme_minimal() + labs(x="Dimension", y="Energy (J)")
@

We should conclude from this chart that the exploration/exploitation balance is not really affecting the amount of energy spent. However we would like to know if there is some correspondence between the amount of extra energy spent and the fitness achieved. We show this in Figure \ref{fig:evoapps.fitness.vs.energy}.

<<evoapps.fitness.vs.energy, echo=FALSE, warnings=FALSE, message=FALSE, fig.height=3, fig.pos="h!tb", fig.cap="Energy vs fitness difference to the optimum for all experiments; point size is related to the max number of generations without change; fill color is blue for dimension =5, red for dimension = 3; shapes are related to population size and finally point size to the number of generations without improvement. Lower is better for fitness value">>=
evoapps_results$log_diff <- log10(evoapps_results$diff_fitness)
evoapps_results$dimension <- as.factor(evoapps_results$dimension)
ggplot(evoapps_results, aes(x=delta_PKG,y=log_diff)) + geom_point(,fill=evoapps_results$color,size=evoapps_results$point_size,shape=evoapps_results$shape) + theme(legend.position="none") + labs(x="Energy (J)", y="log(diff. target)")
@

In this chart, that follows the usual convention, we see that in experiments with dimension is equal to 5 very little energy is spent, but in fact fitness level is not very good. Triangles (population size = 200) seem to need less energy, or at least a more consistent amount of energy, to achieve a certain value. Finally when too many generations without change are used, that will result in general in more energy needed for achieving a fitness level. What we want, however, for this chart to move left (less energy) and down (better fitness), if possible at the same time. However, that will not generally be possible, so we will have to settle with approaching every one of the problems in turn.

We will have to take into account the characteristics of the language to do so. Julia is a garbage-collected language \cite{de2025reconsidering}, implying that it will stop from time to time to collect unused pointers created by allocated memory that is no longer used. In most cases, optimizing code looks to reduce the amount of allocations, so that the amount of time devoted to it is reduced.

We need to find such a target for optimization, however, and we found it in the mutation operator. Mutation is applied every time a new individual (after the initial population) is generated, that is, several thousand times in this kind of experiment; reducing the amount of allocations it uses will obviously have an impact. This is what we will do next.

<<evoapps.perf.boost, echo=FALSE, warnings=FALSE, message=FALSE, fig.height=3, fig.pos="h!tb", fig.cap="Energy vs time for all experiments; point size is related to the max number of generations without change; fill color is white for baseline, black for mutation with improved performance; shapes are related to population size and finally color to problem dimension">>=
evoapps_perf_results <- read.csv("data/sphere-evoapps-fix-bna-13-Oct-18-54-04.csv")
evoapps_perf_results$work <- "BNA-mutation-perf-boost"
evoapps_perf_results$delta_PKG <- 0
evoapps_perf_results$delta_seconds <- 0

for (dim in c(3,5)) {
  for ( pop_size in c(200,400)) {
    number_of_rows <- nrow(evoapps_perf_results[ evoapps_perf_results$dimension==dim & evoapps_perf_results$population_size==pop_size,])
    evoapps_perf_results[ evoapps_perf_results$dimension==dim & evoapps_perf_results$population_size==pop_size,]$delta_PKG <-
      evoapps_perf_results[ evoapps_perf_results$dimension==dim & evoapps_perf_results$population_size==pop_size,]$PKG  -
      rep(summary_baseline_results[ summary_baseline_results$population_size == pop_size & summary_baseline_results$dimension==dim, ]$median_energy,number_of_rows)

    evoapps_perf_results[ evoapps_perf_results$dimension==dim & evoapps_perf_results$population_size==pop_size,]$delta_seconds <-
      evoapps_perf_results[ evoapps_perf_results$dimension==dim & evoapps_perf_results$population_size==pop_size,]$seconds  -
      rep(summary_baseline_results[ summary_baseline_results$population_size == pop_size & summary_baseline_results$dimension==dim, ]$median_time,number_of_rows)
  }
}
evoapps_perf_results$population_size <- as.factor(evoapps_perf_results$population_size)
evoapps_perf_results$point_size <- ifelse(evoapps_perf_results$max_gens==10,2,4)
evoapps_perf_results$shape <- ifelse(evoapps_perf_results$population_size==200,21,24)
evoapps_perf_results$color <- ifelse(evoapps_perf_results$dimension==3,"red","blue")
evoapps_perf_results$log_diff <- log10(evoapps_perf_results$diff_fitness)
evoapps_perf_results$dim_pop <- paste0("D:",evoapps_perf_results$dimension," Pop:",evoapps_perf_results$population_size)

all_results <- rbind(evoapps_perf_results, evoapps_results)

all_results$fill <- ifelse(all_results$work=="bna","white","black")
ggplot(all_results, aes(x=delta_seconds,y=delta_PKG)) + geom_point(shape=all_results$shape,alpha=0.5,fill=all_results$fill,size=all_results$point_size,color=all_results$color) + theme_minimal() + labs(x="Time (s)", y="Energy (J)")

results_anova_pkg <- aov( lm ( PKG ~ work + dimension + population_size + max_gens + alpha, data=all_results ) )
results_anova_result_3 <- aov( lm ( log_diff ~ population_size + max_gens + alpha, data=all_results[ all_results$dimension == 3,] ) )
results_anova_result_5 <- aov( lm ( log_diff ~ population_size + max_gens + alpha, data=all_results[ all_results$dimension == 5,] ) )
results_anova_boost <- aov( lm ( PKG ~ dimension + population_size + max_gens + alpha + work, data=all_results ) )
@


% The results of the experiments have been summarized in Figure \ref{fig:walcom.results}, which plots energy consumption in Joules vs time spent, representing problem dimension with different shapes and population size with different colors. It is interesting to note that problem dimension is not really a factor in energy consumption, since the algorithm runs until the maximum number of generations without change is reached; when the problem dimension is higher, it simply stops in a worse solution. Most points, however, independently of the problem dimension or other factors, are in the 400-450 Joules and 8-8.5 seconds range. At the end of the day, this means that any optimization we might be able to make in this specific problem will, at most, be in the 10\% range in the average case. This is why in most cases optimization will should focus on the worst case scenario, as we will see later.

% This is an usual result, but we are more interested in checking what is the dependence of energy consumed on the different parameters of the algorithm. In order to check this, we have performed an ANOVA analysis of the results, which yields that the variation is significant below the 0.05 threshold for 3 of the 4 components: all but the maximum number of generations, which is used as a stopping criterion. Increasing the number of generations that the algorithm will run without finding a better solution seems to be mostly {\em free}, giving the algorithm designer a certain amount of freedom when choosing this parameter. This price might not be extended to other sizes or problems, so as is usual in the energy profiling of any metaheuristic, it is advisable to perform a specific analysis for each problem and size. Performing an analysis on the fitness obtained, the results are similar: there is no significant dependence on the result achieved.
%
<<evoapps.evaluations, echo=FALSE, warnings=FALSE, message=F, fig.height=3, fig.pos="h!tb", fig.cap="Energy consumption vs number of evaluations">>=
ggplot(all_results, aes(x=evaluations,y=PKG,color=work)) + geom_point(alpha=0.5) + theme_minimal() + labs(x="Evaluations", y="Energy (J)")
all_results$joules_per_eval <- all_results$PKG / all_results$evaluations
ggplot(all_results, aes(x=work,y=joules_per_eval))+ geom_boxplot(notch=T) + theme_minimal() + labs(x="Work", y="Joules per evaluation")
comparison <- wilcox.test(all_results$joules_per_eval[ all_results$work == "BNA-baseline"], all_results$joules_per_eval[ all_results$work == "BNA-mutation-perf-boost"])
energy_anova_boost <- aov( lm ( joules_per_eval ~ dimension + population_size + work, data=all_results ) )
@

<<walcom.models, echo=FALSE, message=F, warning=F>>=
library(kableExtra)
evaluations_model <- glm(evaluations ~ dimension + population_size + max_gens + alpha, data=all_results)

all_results %>% group_by(dimension,population_size,max_gens,alpha,work) %>%
  summarise(mean_energy=mean(PKG), sd_energy=sd(PKG),
            mean_time=mean(seconds), sd_time=sd(seconds),
            mean_evaluations=mean(evaluations), sd_evaluations=sd(evaluations),
            mean_generations=mean(generations), sd_gens=sd(generations),
            mean_diff_fitness=mean(log_diff), sd_diff_fitness=sd(log_diff)) -> summary_results

summary_results$energy <- paste0(round(summary_results$mean_energy,2), " (", round(summary_results$sd_energy,2), ")")
summary_results$time <- paste0(round(summary_results$mean_time,2), " (", round(summary_results$sd_time,2), ")")
summary_results$evaluations <- paste0(round(summary_results$mean_evaluations,2), " (", round(summary_results$sd_evaluations,2), ")")
summary_results$generations <- paste0(round(summary_results$mean_generations,2), " (", round(summary_results$sd_gens,2), ")")
summary_results$diff_fitness <- paste0(round(summary_results$mean_diff_fitness,2), " (", round(summary_results$sd_diff_fitness,2), ")")

summary_results <- summary_results[,c("dimension","population_size","max_gens","alpha","work","energy","time","evaluations","generations","diff_fitness")]
kable(summary_results, digits=2, booktabs=T, col.names = c( "D","Pop. size", "Gens.", "Alpha %", "Mutation op", "Energy", "Time (s)", "Evals", "Gens.", "log(diff. target)"),  caption="Summary of results") %>% kable_styling(full_width=F)
@

The whole table of results is shown in Table \ref{tab:walcom.models}, showing the average and standard deviation for the measures obtained for every combination of values: the average energy consumed in Joules, time in seconds, number of evaluations until completion, how many generations are needed, and finally the logaritm of the difference between the fitness objective and the best fitness found.

<<walcom.dimensions, echo=FALSE, message=F, warning=F, fig.height=4, fig.cap="Box plots of energy spent for the different dimensions">>=
ggplot(all_results, aes(x=dimension, y=PKG, fill=work)) + geom_boxplot( notch=T) + theme_minimal() + labs(x="Dimension", y="Energy (J)", title="Energy distribution by dimension") + theme(legend.position="none") + scale_y_log10()
@

Since the main objective of this paper was not to optimize the performance of the algorithm, we clearly observe that the fitness results for the Sphere function and the problem with 5 dimensions are not really competitive. This might explain why, in general, all dependent results are inferior: there are less evaluations, it takes less time on average and it stops sooner. We can obviously reverse this result: you normally need more energy to find better solutions. Please check Figure \ref{fig:walcom.dimensions} for a boxplot that compares energy needed for 3 and 5 dimensions, showing how the median is higher for the smaller problem, but again it is mainly because they explore better the solution space, finding better solutions. In this paper we were more interested in the exploration/exploitation balance for specific problem sizes, so we will analyze this in more detail for every problem size separately.

<<walcom.violin, echo=FALSE, message=F, warning=F, fig.height=3, fig.show='hold', fig.cap="Violin plots for the different dimensions, algorithm parameters and population size.">>=

for (dim in c(3,5)) {
  dim_results <- all_results[ all_results$dimension == dim,]
  p1 <- ggplot(dim_results, aes(x=population_size, y=PKG, fill=work)) + geom_violin() + theme_minimal() + labs(x="Population size", y="Energy (J)", title=paste("Energy distribution for dimension", dim))
  print(p1)
  dim_results$alpha <- as.factor(dim_results$alpha)
  p2 <- ggplot(dim_results, aes(x=alpha, y=PKG, fill=work)) + geom_violin() + theme_minimal() + labs(x="Alpha", y="Energy(J)", title=paste("Energy distribution for dimension", dim)) + theme(legend.position="none")
  print(p2)
}

@

Figure \ref{fig:walcom.violin} shows violin plots for the different dimensions, with the intention of comparing how different parametrizations spend energy. The top two plots refer to the problem with dimension = 3. The interesting thing in violin plots is that we can see more clearly the worst case scenario, with parts of the "violin" indicating the amount of cases that fall in that range; in that sense, we can see that in the worst case scenario, population 200 is worse than 400 and that an $\alpha$ percentage of 10\% is better than 25\% since it concentrates most of its results in the 400-450 Joules area. $\alpha$ = 25\% has a wider distribution does have many results in the low-400 Joules area, however there are relatively more cases that fall in the 500 Joules range or beyond. We will have to conclude in this case that we need to keep exploitation to a low rate with a {\em small} $\alpha$ caste, coupled, in this case, with a large population.

The problem with dimension = 5 behaves in the same way with respect to the $\alpha$ percentage: higher is worse, making it more likely to spend too much energy; however, the behaviour with respect to the population is totally different, with more population making it more likely to spend more energy in the worst case scenario.

<<walcom.energy, echo=FALSE, message=F, warning=F, fig.height=4, fit.pos="h!tb",fig.cap="Result as a function of energy spent for all experiments. Point size is related to the alpha caste percentage. Lower is better in the y axis.">>=
all_results$point_size <- ifelse(all_results$alpha==10,2,4)
ggplot(all_results, aes(x=PKG,y=log_diff,color=dimension)) +
  geom_point(alpha=0.5,size=all_results$point_size,shape=all_results$shape,fill=all_results$fill) +
  theme_minimal() + labs(x="Energy (J)", y="log(diff. target)")+
  scale_x_log10()
@

Finally, Figure \ref{fig:walcom.energy} tries to summarize the results, plotting the quality of the solution found (lower is better) as a function of energy spent in Joules. It again shows clearly that the problem with dimension 5 arrives to worse solutions, grouped in the upper part of the graph. If we look at these, we see that the energy spent to arrive more or less at the same solutions is higher when the population is higher. This points to a low diversity in the population, and a bad exploration/exploitation balance. From the point of view of energy spent, increasing the exploration capability by increasing the percentage of population in the $\alpha$ and $\beta$ castes might increase the energy spent, and in some cases obtain better solutions, leading to the best case of all experiments. However, the most important takeaway here is that there does not seem to be a correlation between energy spent and better solutions, and that tweaking the exploration/exploitation balance will not affect energy spent, having possibly a positive effect on the solutions. At any rate, solutions are suboptimal so this needs to be explored further.

This trend is probably more visible for problem dimension = 3, in the lower part of the chart. The best solutions use $\alpha = 25\%$ (with, correspondingly, $\beta = 50\%$), tipping the balance slightly more towards exploitation. But the outstanding part is that better solutions are obtained without implying bigger energy expenses, on the contrary, energy spent is around 400 Joules with few outliers.

This might be, in part, related to the implementation in the language Julia. Julia is extremely fast, which implies that the application of different operators has a negligible difference. The actual impact on the energy consumed must have a different origin.

%<<walcom.evals, echo=FALSE, message=F , warning= F,  error=F, fig.height=5, fig.cap="Energy consumption as a function of evaluations for all experiments. %Point size is related to the alpha caste percentage. Lower is better in the y axis.">>=
%summary_pkg <- summary(lm( PKG ~ evaluations * generations, data=baseline_results))
%ggplot(baseline_results, aes(x=evaluations,y=PKG,color=generations)) + geom_point(alpha=0.5) + theme_minimal() + labs(x="Evaluations", y="Energy (J)")
%

%In Figure \ref{fig:walcom.evals} we plot the energy consumed by PKG vs. the number of evaluations, with the color related to the number of generations needed. A linear model shows significant dependence of energy consumed with the number of evaluations, the number of generations and also the interaction between the two variables. This dependence goes in an unexpected direction: energy spent decreases with the number of evaluations, but increases with the number of generations; in the chart, we can see that as we go to higher energy consumption values, the points are lighter, that is, the algorithm has run for more generations. This would encourage us to create populations that are as small as possible, and cramming as many evaluations as possible into every one of them, putting more elements of the population into the $\epsilon$ caste, which is the one that performs local search.


\section{Conclusion and discussion}
\label{sec:discussion}

In this paper we set out to analyze the influence of parameters that control the exploration/exploitation balance in a population based metaheuristic called Brave New Algorithm, implemented in a language called Julia. Using a methodology that follows best practices, we have measured energy spent by the algorithm parametrized in a series of different ways that affect the exploration/exploitation balance, in order to obtain rules that allow any user to minimize the amount of energy spent. These experiments, however, have yielded mixed results, the main of which being that parametrization needs to take care first of obtaining a good solution, and then, next, on minimizing the amount of energy. The exploration/exploitation balance is tricky in that sense, because it will affect the number of evaluations needed to find a solution with a certain level, but also the amount of energy consumed.

In our experiments, however, what we find is that maintaining a low percentage of {\em exploitative} population will lead, if not to an overall lower energy consumption, it will minimize the probability of spending too much energy to find a solution. This might have to do with the fact that  the "explorative" operators will consume {\em less} energy to arrive to a solution than the equivalent {\em explorative} operators, which need two operands, as well as more interchange of information. Even if the difference per call is small, these operators are called many times so it might add up to a considerable difference. This effect will add to the fact that, in general, a lower degree of exploitation is the usual practice in evolutionary algorithms such as this one.

A matter of discussion would be the best way to eliminate the overhead in this case. The main issue here is that if the overhead is relatively high, the differences between different parametrizations might be compressed and thus will become much more difficult to detect. If we simply take into account the wallclock time it takes to run every script, simply the compiler overhead might have double digits. We will need to take that into account in the future.

A different issue is also the optimality of the operators used; real-valued optimization has a host of different mutation and crossover operators, out of which the most popular is probably the Gaussian mutation operator. Although from the point of view of the energy spent the difference with a random mutation operator might be small, the time it would take to reach the solution would probably be smaller, which of course matters for the overall energy spent. This operator, however, is more {\em exploitative}, since it will generate solutions around those that are already there, so there should be a different way of adding a explorative component. Fortunately, the caste system used in BNA allows for different operators used in different parts of the population, so it would be relatively easy to accommodate. This is proposed as a future line of work, together with other operators such as BLX-$\alpha$ or arithmetic crossover operators. Once again, different version of the operator could be applied to different castes.

Other future lines of work will take into account that different parametrizations are needed for different problems and sizes, so a more fair comparison of how resource consumption increases with size will need to perform some kind of parameter optimization for every size and problem, since at the end of the day what a scientist needs is to minimize the energy consumption for a solution quality level. An important issue that needs to be addressed is to take into account the overhead introduced by the compiler every time a Julia script is run, as well as the overhead of the rest of the system.


\section*{Acknowledgements}
% This work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y Competitividad (Spanish Ministry of Competitivity and Economy) under project PID2023-147409NB-C21.
Acnowledgements taking\\
this much space

\bibliographystyle{splncs04}
\bibliography{ours,energy,ga-energy,GAs,julia,metaheuristics}


\end{document}
