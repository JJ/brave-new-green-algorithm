\documentclass[10pt,conference]{IEEEtran}
\usepackage[T1]{fontenc}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{color}
\usepackage{url}

\begin{document}

\title{}


\author{
\IEEEauthorblockN{JJ Merelo}
\IEEEauthorblockA{Department of Computer Engineering, Automatics and Robotics\\
University of Granada\\
Email: jmerelo@ugr.es\\
CITIC, UGR, Spain\\ORCID:0000-0002-1385-9741}
\and
\IEEEauthorblockN{Cecilia Merelo-Molina}
\IEEEauthorblockA{Zenzorrito, Granada, Spain}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
As we proceed with incorporating energy efficiency into the design of our algorithms, establishing a robust methodology for energy profiling and the eventual comparison of algorithms or implementations is essential for a solid foundation for any further work. The main issue to overcome is the lack of specific per-process measurement of energy consumption, and above that, the fact that we are going to be measuring a system that is actively trying to optimize energy consumption itself, and doing so for the whole system, above and beyond the workload we are trying to measure. In this paper, we propose a two-stage methodology for energy profiling in population-based metaheuristics. The first stage will consist of an experimental design, i.e., how to run a series of experiments to measure the energy consumption of a specific configuration and implementation of the algorithm. The second phase will consist of the statistical processing of these results to obtain a measure of energy consumption with a certain degree of certainty. Finally, we will apply these measurements to a type of evolutionary algorithm, called the Brave New Algorithm, written in the Julia language, to determine the impact of changes at different levels on energy consumption.

\end{abstract}

\begin{IEEEkeywords}
Green computing, Energy profiling, Metaheuristics
\end{IEEEkeywords}

\section{Introduction}

For the last few years, metaheuristics and machine learning algorithms have been achieving new levels of performance thanks to the use of more powerful processors, more powerful architectures and a greater amount of on-board memory. We already know that there is no free lunch, and this has been done at the expenses of increasing energy consumption. One of the languages that is more widely used in machine learning and scientific computing in particular, Python, is known to be one of the most energy-consuming ones \cite{a18090593,Pythoncpp,nahrstedt2024empirical}, but there are decisions that need to be taken at many different levels, from algorithm parametrization down to the language and architecture used in it, whose energy consumption needs to be assessed.

Whether you want to present the reduction of energy consumption in algorithms implementation as an engineering requirement or as an ethical objective \cite{MoralDesignandGreenTechnology}, the need of a methodology for comparing different configurations so that a decision can be reached is quite clear. This methodology must take into account that, nowadays, scientists seldom devote exclusive resources to running their experiments, relying on personal systems in most cases. Any measurement methodology should work on these systems giving reliable or at least actionable results.

Additionally to this overhead that should be eliminated to consider only the workload we are interested in, we should take into account that modern computers rely on automatic power management techniques that are working directly on the processor or the chipset; these processes will on one hand help reduce energy consumption, but on the other it will make it more difficult to measure, since it might be working in the same direction as the optimization, or maybe the opposite, if whatever micro-optimization is introduced happens to disconnect the heuristics the system itself is utilizing; besides, the system is trying to do several things at the same time: keeping the system temperature below a given level, and lowering power consumption without reducing performance. This creates a very high variability.


The rest of the paper is organized as follows: the next Section describes the state of the art in the exploration of energy consumption of evolutionary algorithms and other metaheuristics. Section \ref{sec:bna} describes the algorithm and how it has been parametrized for the purpose of this paper. Since there is no fixed methodology for measuring energy consumption, we will show how it was done for this experiment and the trade-offs it implies, together with the results, in Section \ref{sec:results}; these results will be discussed in the last Section \ref{sec:discussion} along with our conclusions.


\section{State of the art}

Despite the growing interest in green computing, the challenges of measuring with a certain degree of accuracy the energy consumption of a particular workload have not been addressed so thoroughly. Von Kistowski et al. \cite{von2018measuring} introduces a methodology, called "SPEC power", which takes into account the CPU load and enables to measure the power consumed by the workload under study. The framework is rather geared towards measuring whole systems, since it uses an external, calibrated, power meter. However, working on a real system, with other processes running, would require a totally different approach. Freina et al. \cite{freina2024survey} present a more comprehensive survey of the challenges involved in using hardware or in-system software APIs to measure the amount of energy consumed by a certain workload.


\section{A Brave New Algorithm}\label{sec:bna}

The Brave New Algorithm \cite{merelo2022brave} is essentially an evolutionary algorithm, using the usual mechanism of mutation, crossover and selection to find the optimum of a given function, represented as a {\em chromosome}, in this case a vector of real numbers. However, how selection and reproduction take place is similar to how the novel Brave New World \cite{huxley2022brave} organized society: in castes. \begin{itemize}
\item $\alpha$ generates new members by coupling the best individuals among them, to then undergo mutation and crossover.
\item $\beta$ caste needs to reproduce a member of the $\alpha$ caste, and another member of its own caste, to then undergo mutation and crossover.
\item $\delta$ and $\epsilon$ castes generate new individuals by mutation. The only difference between them is that those who reproduce are chosen among the other members of the population, and that mutation rates can be set separately.
\item $\gamma$ generates new members by mutation, but mutation might be applied several times while the new individual is better than the previous one.
\end{itemize}

The operators used are mutation, that will change 40\% of the elements of the vector generating a new, random element, 2-point crossover, and selection via binary tournament \cite{blickle1996comparison}. After every generation the population is re-ranked again, with the first \%$\alpha$ of the population assigned to that caste, and so on; through generations every caste will hold the same number of individuals. Since the two {\em upper} castes are mainly devoted to exploitation and the rest is devoted to exploration, the proportion of individuals in them is the main parameter governing the balance.
There are some restrictions in these percentages, due to the way new members are generated: the beta caste needs to have twice as many members as the alpha caste; the percentage of population in the $\alpha$ caste is, thus, the main parameter for this, since we can use it to generate the rest. In practice, what we have done is to vary the percentage of the $\gamma$ caste, the "first" that performs local search, while keeping $\delta$ and $\epsilon$ fixed to a low value.

The implementation of the algorithm used is available under a free license at \url{https://github.com/cecimerelo/BraveNewAlgorithm.jl}

\section{Experimental methodology and results}\label{sec:results}

In order to carry out these experiments, we have used a methodology that is like that used in our previous papers: \cite{low-level}. We employ {\sf pinpoint}, \cite{pinpoint}, a command line tool that taps the RAPL interface \cite{rapl} to measure energy consumption of a given process. The version we have used was compiled from commit {\tt dfee658}. This tool was validated in \cite{icsoft} by comparing its output to other energy-measuring tools and finding that it was less error-prone than the rest, and gave measures that were consistent with the tools such as {\tt perf} and {\tt likwid} when they did not fail. The combination of the RAPL API, which places estimates in a register, and the overhead by the tool cannot be avoided, however, which is why we have to apply a careful experimental design to mitigate adverse effects. The sampling frequency we use for the RAPL counters is 10 Hz, that is, 1 sample every 100 miliseconds.

We carry experiments in an AMD Ryzen 9 9950X 16-Core Processor, with Ubuntu Linux 25.04 with kernel version 6.14.0-29-generic. Please note that AMD implements a version of the RAPL API which only allows the measurement of a single value for the processor, PKG or package, excluding memory and different components within the package. Although useful, diving into memory consumption or the consumption of other parts of the processor would be more interesting for micro-optimization, or explaining the results achieved, than for overall assessment or reduction of energy consumption, which is what we are doing in this paper. Julia version has been $1.11.7$ except when noted.\footnote{Julia has now changed the minor version to 1.12, with $1.12.1$ being the latest released at the moment of writing this paper. However, there are some performance issues with this new version, which has prevented us for using it for the time being.}

We have used the Sphere function \cite{hansen2010comparing}, defined as sum of the squares of the distance to the center minus a fixed value, which is part of the BBOB benchmarks; as a matter of fact, the implementation of this function is taken from the {\sf BlackBoxOptimizationBenchmarking.jl}, also written in Julia. This function is also lightweight enough to allow us to compare the energy profiles of different combinations of genetic operators, which will then have a bigger impact on overall consumption.

Our initial experimental design includes two problem sizes: vectors with 3 or 5 dimensions, to check the impact of the parameters for different problem difficulties. This corresponds to the low end of the BBOB benchmark suites; remember that we are using this suit just for the purpose of profiling the energy for this language and algorithm, so for the time being we did not find it necessary to go into higher dimensions.

\begin{itemize}
\item Population size: 200 or 400 individuals, which has an impact on the diversity of the population and thus the exploitation capabilities of the algorithm.
\item Maximum number of generations without improvement: 10 or 50, which is the stopping criterion for the algorithm. In this paper we have made this specific choice for stopping criterion, because if the algorithm tends towards exploration, it will explore in fruitless directions generating (possibly) useless chromosomes that will not allow it to escape a local minimum; in particular, this will have the consequence that depending on the problem dimension the execution might fall well short of an optimum value. We have left it this way, however, due to its direct relationship with exploration.
\item Percentage of population in the $\alpha$ caste: 10\% or 25\%, which is the main parameter that controls the exploration/exploitation balance, since only the $\alpha$ and $\beta$ castes perform exploitation. The rest of the parameters will be set accordingly: $\beta$ percentage will double that value. $\gamma$ caste will hold a proportion equal to the $\alpha$ and $\beta$ percentages subtracted from 90.
$\delta$ and $\epsilon$ castes will always have the same proportion, each equal to 5\%.
\end{itemize}

\begin{table}[h!tb]
\centering
\caption{Caste percentages depending on the $\alpha$ proportion used.}\label{tab:params}
\begin{tabular}{lcc}
\toprule
$\alpha$ percentage & Exploration proportion & Exploitation proportion \\
\midrule
10\% & 30\% & 70\% \\
25\% & 75\% & 25\% \\
\end{tabular}
\end{table}

This means that the $\alpha$ percentage is the main one governing this exploration/exploitation balance. Please check table \ref{tab:params} for how the explorative/exploitative castes are organized depending on the values used.
Every parametrization runs 30 times sequentially in a single core. Results written in standard output are processed and are available from this paper's repository at \url{https://github.com/JJ/brave-new-green-algorithm}.



\section{Conclusion and discussion}
\label{sec:discussion}


\section*{Acknowledgements}
This work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y Competitividad (Spanish Ministry of Competitivity and Economy) under project PID2023-147409NB-C21.

\bibliographystyle{IEEEtran}
\bibliography{ours,energy,ga-energy,GAs,julia,metaheuristics}

\end{document}
