\documentclass[10pt,conference]{IEEEtran}
\usepackage[T1]{fontenc}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{color}
\usepackage{url}
\usepackage{amsmath}

\begin{document}

\title{Can we measure energy consumption in population-based metaheuristics?}

\author{
\IEEEauthorblockN{A. N. Onymous-Authors}
\IEEEauthorblockA{Affiliation\\
Email:
Hidden place and ORCID}

% \IEEEauthorblockN{JJ Merelo}
% \IEEEauthorblockA{Department of Computer Engineering, Automatics and Robotics\\
% University of Granada\\
% Email: jmerelo@ugr.es\\
% CITIC, UGR, Spain\\ORCID:0000-0002-1385-9741}
% \and
% \IEEEauthorblockN{Cecilia Merelo-Molina}
% \IEEEauthorblockA{Zenzorrito, Granada, Spain\\
% ORCID: 0000-0002-5902-0159}
}
%
\maketitle
%
\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
Green computing, Energy profiling, Metaheuristics
\end{IEEEkeywords}

\section{Introduction}

Concerns regarding the energy footprint of algorithm implementations are well-established \cite{sinha2001jouletrack,kurp2008green,lewis2008run,aggarwal2015greenadvisor,kwon2013reducing} since the beginning of the century; its urgency, however, has intensified with the proliferation of resource-intensive metaheuristics \cite{irace,novoa2021measuring}, deep learning, and large language models \cite{10549890,rodriguez2024evaluatingenergyconsumptionmachine}. Consequently, the focus in software engineering has shifted from optimizing for performance toward {\em greening} algorithm implementations. While performance benchmarking and improvement has well-established methodologies and tools, optimizing energy consumption is still lacking in both areas. For starters, measuring energy consumption of the implementation of an algorithm involves an experimental framework where configurations are compared by measuring their energy consumption in specific environments and applying statistical techniques to identify the most efficient parameters for a given workload, eliminating any system and implementation overheads.

These stages, measurement and statistical comparison, are critical but fraught with technical challenges. The primary obstacle is that, unlike traditional performance metrics, it is impossible to isolate the energy consumption of a single process running on a specific core under a modern operating system (OS). Since hardware sensors and counters operate system-wide, researchers must synchronize these measurements with process execution and subsequently attempt to isolate the system overhead. Standard practice involves taking baseline measurements to account for OS and framework overhead, then subtracting these from workload measurements to estimate the "real" energy consumed, but no matter what you do, you will still measure what the device is consuming {\em while your process is running in it}, not the extra power the specific core has required from the source or how much energy has spent while in it. % We need to follow up on this later

However, a second, there exists a more determinant challenge to obtaining the {\em true} amount of energy an algorithm implementation spends: the influence of the operational context. Processor states and operating conditions, particularly temperature, influence energy consumption in a non-linear fashion. This affects not only the absolute baseline but also the relative differences between the baseline and the active workload \cite{freina2024survey,cruz2025}. Modern chipsets and OSs actively manage thermal envelopes through Dynamic Voltage and Frequency Scaling (DVFS) and Dynamic Power Management (DPM) \cite{snowdon2005power}; additionally, some modern processors have heterogeneous cores dividing them between high-performance "P" or "Performance" cores and energy efficient "E" cores. The OS will assign a new process to core with varying voltage and frequency depending on un predictable conditions. For scientists working on non-dedicated personal computers running personal software besides the algorithm implementation that is the object of the measurement, the system state is in constant flux. Power reduction measures and thermal throttling are triggered not just by the workload under study, but by background processes and OS core election policies, which may move tasks between "performance" and "efficiency" cores unpredictably.

One could attempt to mitigate this by strictly controlling the system state, but projecting such results to "real-world" operating conditions is problematic. The interaction between system state and workload is often unknown; a configuration that appears optimal in a cooled, idle state may perform poorly when interacting with a thermally saturated system or a different core allocation.

This necessitates a new methodological approach: one that explicitly takes into account the relationship between CPU package temperature, core election and its consequences on performance, and energy measurements. The goal of this paper is to propose and test methodologies that minimize the influence of these environmental variables. Through a series of extensive experiments, we analyze how temperature, as captured by internal sensors, interacts with workload implementation. We are going to work in a processor with two dice, and test different tactics of assigning processes to dies depending on temperature, with the main intention of trying to minimize the influence of temperature and core election (measured by runtime) in energy measurements for the workload under study, in such a way that higher-precision comparisons can be made between two different configurations of the workload. These comparisons are essential to evolve software in such a way that its carbon footprint can be minimized. At the same time, the measurement methodology could be extended to an actual production methodology so that energy spent in experimental runs can also be minimized.

The rest of the paper is organized as follows: next we will review the state of the art mainly focused in methodologies for energy measurement and heuristic assessment of the influence of the operating environment in energy consumption. We will next present briefly the algorithm we are going to apply measurements for, called Brave New Algorithm in Section \ref{sec:bna}. Section \ref{sec:results} will present the experimental methodology and results, and finally we will discuss the results and present conclusions in Section \ref{sec:discussion}.

\section{State of the art}

Despite the growing imperative for green computing, achieving high accuracy in measuring the energy consumption of specific workloads remains a methodological hurdle often acknowledged but seldom resolved \cite{10549890}. The foundational "SPEC power" methodology \cite{von2018measuring} provides a framework for measuring whole-system power consumption relative to CPU load; however, because it relies on external, calibrated power meters, it is less suited for isolating granular software workloads in non-dedicated environments where background processes compete for resources.

Addressing this gap, Freina et al. \cite{freina2024survey} categorized hardware and in-system software APIs, noting that tools leveraging model-specific registries (MSRs) offer a compelling balance of availability and precision. These software-based solutions exhibit error margins below 5\% \cite{6557170} with a negligible overhead of approximately 1\%. While a 10Hz to 20Hz sampling frequency is generally advised for temporal granularity, these tools alone do not constitute a methodology. The guiding principles of reproducibility, fairness, verifiability, and usability \cite{von2018measuring}—originally intended for data centers—must be adapted for the highly variable conditions of local execution environments.

The most significant methodological advancements have emerged within the field of evolutionary algorithms and metaheuristics. A critical discovery is the {\em hysteretic effect} of system state: the system often remains in a high-energy-consumption state long after a workload has finished \cite{cotta25}. This phenomenon is largely driven by thermal dynamics; the temperature reached during one experimental run significantly biases the next \cite{cotta24}. While earlier models suggested a quadratic relationship between temperature and power consumption in the 20°C to 50°C range \cite{devogeleer2014modelingtemperaturebiaspower}, more recent analyses indicate an exponential relationship as temperatures approach 80°C \cite{PENG2025104837}.

To mitigate these thermal and state-based biases, several experimental strategies have been proposed:\begin{itemize}

\item {\em Thermal Stabilization:} Implementing a "sleep" phase after every measurement to allow the system to cool \cite{cotta25}. However, this remains difficult to guarantee in real-world environments where background processes are beyond the researcher's control.
\item {\em Statistical Validation (R3-Validation):} The "Robin and Rotate" (R3) approach \cite{rigorous} attempts to achieve reproducibility by running cycles of experimental variants in shifting orders. This design aims to mitigate changing system states and background consumption () without requiring a system reboot between every run.
\item {\em Hardware-Level Design:} Recent research into new core layout algorithms \cite{PENG2025104837} demonstrates that energy concerns are being tackled at the architectural level, though such solutions are generally inaccessible to software researchers looking to optimize existing algorithm implementations.
\end{itemize}

Ultimately, while experimental design can mitigate some "noise", a gap remains in modeling how the operational context—specifically temperature and OS-level core election—directly biases the comparison of algorithm configurations. This creates a need for methodologies that do not just rotate experimental order, but explicitly account for the non-linear influence of thermal states on the resulting energy data.

\section{A Brave New Algorithm}\label{sec:bna}

The Brave New Algorithm \cite{merelo2022brave} is essentially an evolutionary algorithm, using the usual mechanism of mutation, crossover and selection to find the optimum of a given function, represented as a {\em chromosome}, in this case a vector of real numbers. However, how selection and reproduction take place is similar to how the novel Brave New World \cite{huxley2022brave} organized society: in castes. \begin{itemize}
\item $\alpha$ generates new members by coupling the best individuals among them, to then undergo mutation and crossover.
\item $\beta$ caste needs to reproduce a member of the $\alpha$ caste, and another member of its own caste, to then undergo mutation and crossover.
\item $\delta$ and $\epsilon$ castes generate new individuals by mutation. The only difference between them is that those who reproduce are chosen among the other members of the population, and that mutation rates can be set separately.
\item $\gamma$ generates new members by mutation, but mutation might be applied several times while the new individual is better than the previous one.
\end{itemize}

The operators used are mutation, that will change 40\% of the elements of the vector generating a new, random element, 2-point crossover, and selection via binary tournament \cite{blickle1996comparison}. After every generation the population is re-ranked again, with the first \%$\alpha$ of the population assigned to that caste, and so on; through generations every caste will hold the same number of individuals. Since the two {\em upper} castes are mainly devoted to exploitation and the rest is devoted to exploration, the proportion of individuals in them is the main parameter governing the balance.
There are some restrictions in these percentages, due to the way new members are generated: the beta caste needs to have twice as many members as the alpha caste; the percentage of population in the $\alpha$ caste is, thus, the main parameter for this, since we can use it to generate the rest. In practice, what we have done is to vary the percentage of the $\gamma$ caste, the "first" that performs local search, while keeping $\delta$ and $\epsilon$ fixed to a low value.

The implementation of the algorithm used is available under a free license at \url{https://hiddenu.rl}
% \url{https://github.com/cecimerelo/BraveNewAlgorithm.jl}

\section{Experimental methodology and results}\label{sec:results}


This study investigates the heuristic measurement of temperature's influence on energy consumption, an inquiry stemming from research into energy optimization for metaheuristics. Our workload in this paper is an implementation of the BNA explained above. All experiments are executed as single-threaded processes, which are then run in a single core of the processor.

The algorithm is configured to minimize the Sphere function across several parameter configurations, with
problem dimension ($D$) equal 3, 5, or 10, population size ($N$): 200 or 400, and a termination criteria ($G$) equal to 10 or 25 generations without improvement. This last parameter is irrelevant to the measurements, but has been used in the experiments, which is why we list it here.

Each individual run lasts several seconds. A full experimental suite across all configurations takes approximately two hours, providing sufficient time for the system to reach various thermal states and generate detectable differences in energy consumption.

To mitigate environmental noise and baseline energy fluctuations, we employ an interleaved "sandwich" protocol. For every combination of $D$ and $N$, we perform two complete sequences (one for each value of $G$). Each sequence consists of 31 baseline repetitions ($B$)  interleaved with 30 workload repetitions ($W$), following the pattern: $B_1, W_1, B_2, W_2, \dots, W_{30}, B_{31}$. The $B$ configuration only generates the initial population; $W$ runs the full BNA algorithm stopping after $G$ generations without improvement. This design intends to eliminate language overhead, which includes loading the language JIT compiler and the source code and compiling it, from the actual workload measurements, as well as eliminating an one-time operation, the generation of the initial population. The fact that population generation in $B$ includes problem size, a problem variable, as well as population size, an algorithm parameter, these will have a (possibly weak) influence in the measurements, which we will need to assess initially.

The energy consumption for any given workload $W_i$ is calculated by subtracting the average of the two surrounding baselines ($B_i$ and $B_{i+1}$) from the workload measurement. To ensure data robustness across different initial system states, this entire set of experiments is repeated four times at random intervals after system boot, without enforcing a specific "cool-down" period.

Measurements were conducted on a desktop system with an AMD Ryzen 9 9950X 16-Core, 2-die processor. This CPU features a two-die architecture, allowing us to monitor temperature via two distinct sensors ({\tt Tccd1} and {\tt Tccd2}) using the {\tt k10temp-pci-00c3} driver. The operating environment is Ubuntu (Kernel {\tt 6.17.0-14-generic}. We utilize {\sf pinpoint} to tap the RAPL API \cite{pinpoint,rapl,khan2018rapl}. On this AMD architecture, measurements are restricted to Package (PKG) energy. A Perl script (running in superuser mode) automates the execution, processes readings from the `sensors` command, and outputs a CSV file.

All source code and experimental data are hosted in a public GitHub repository: \url{https://github.com/hidd/en}
%\url{https://github.com/CeciMerelo/BraveNewAlgorithm.jl}.

<<icsme.base, echo=FALSE, fig.cap="Baseline energy consumption over running time", fig.env="figure*", fig.height=3>>=
load("data/joint_base_taskset_with_die2.rds")
library(ggplot2)
library(scales)
custom_pow <- trans_new(
  name = "custom_pow",
  transform = function(x) x^0.3,  # Lower power = more stretch at the bottom
  inverse = function(x) x^(1/0.3)
)
ggplot(joint_base_taskset_with_die2, aes(x=seconds,y=PKG, color=initial_temp)) +
  scale_color_viridis_c(trans=custom_pow) +
  geom_point() + theme_minimal() + labs(x="Time (s)", y="Energy Consumption (PKG)", color="Initial Temperature")
@

As a baseline, we have run a series of experiments, in three different batches; every series repeats the 30$W$ +31$B$ for every parameter value.\begin{itemize}
\item First series of 4 experiments, processes are placed in a die by the operating system.
\item Second series of 5 experiments, we use the {\em taskset} command to place processes always on the first die.
\item Third series of 4 experiments, processes placed on the second die.
\end{itemize}

Between different series, a random amount of time and sometimes reboots were made, with the idea of finding the computer in different states. We intend to use this set of experiments as a baseline, and first we will separate all baseline experiments in order to understand the interdependence of temperature, energy consumption and the two independent parameters which could have an influence. The energy vs. time plot is shown in Figure \ref{fig:icsme.base}, where we use color to plot the initial temperature for every process. The roughly linear dependency of energy with time is relatively clear. Time is here a surrogate for power and frequency: when the system alters core frequency to cool it down it will take longer to finish, but then this additional time will cause an increase in energy consumption; same with frequency: when a process is placed in a lower-frequency core by the system (or by us, in the case of the third series) it will take longer and then consume more energy.

<<icsme.model, echo=FALSE, message=FALSE, fig.cap="Model of energy consumption as a function of time and temperature", fig.env="figure*", fig.height=3 >>=
initial_model <- glm(PKG ~ I(initial_temp^2) + initial_temp*dimension*population_size+seconds, data=joint_base_taskset_with_die2)
library(equatiomatic)
extract_eq(initial_model, wrap = TRUE,
           use_coefs = TRUE,
        raw_indices = TRUE,
        terms_per_line = 3,
        swap_var_names = c(
    "initial_temp"  = "T",
    "I(initial_temp^2)" = "T^2",
    "dimension"  = "D",
    "population_size" = "P",
    "seconds" = "R"
  ), label="eq1" )
@

The model is shown in Equation \ref{eq: eq1}, with $R$ equal to running time in seconds. In this case, however, the only significant terms in the model are the coefficients corresponding to $T$, $T^2$ and $R$, which should in general be expected, since the actual mini-workload run by generating the initial population should not need enough energy to make it raise above the noise generated by the operating conditions.

There are two conclusions, however, from this model: first, the fact that using different tactics for placing processes on dies (or cores) does not change the underlying model, which a priori would depend only on the operating environment as well as the program we run; a reading of this is that we will need significant workloads to be able to compare them, although in this case, being only base runs, this could be actually expected, at least for Julia and the BNA which take a relatively long time to compile and due to the complexity of its JIT engine.

Second, the importance of the temperature and running time terms, but also the existence of a non-linear temperature term which is negative, indicating that when temperature rises, its importance in energy consumption decreases. This is probably due to the effect of active measures by the system to cool the CPU by lowering wattage and frequency; this will have the paradoxical effect of increasing energy consumption simply due to the increase in running time.



\section{Conclusion and discussion}
\label{sec:discussion}


\section*{Acknowledgements}
% This work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y Competitividad (Spanish Ministry of Competitivity and Economy) under project PID2023-147409NB-C21.
Supported by a project\\
taking this much space

\bibliographystyle{IEEEtran}
\bibliography{ours,energy,ga-energy,GAs,julia,metaheuristics}

\end{document}
