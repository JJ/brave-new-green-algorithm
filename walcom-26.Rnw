% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{color}

\begin{document}
%
\title{Analyzing how the exploration/exploitation trade off in biologically-inspired algorithms affects energy consumption}
%
\titlerunning{Energy consumption of exploration/exploitation balance}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{JJ Merelo\inst{1,2}\orcidID{ 0000-0002-1385-9741 } \and
Cecilia Merelo-Molina\inst{3}}
%
\authorrunning{Merelo and Merelo}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Department of Computer Engineering, Automatics and Robotics, University of Granada \email{jmerelo@ugr.es}\and
CITIC, UGR, Spain
\and
Zenzorrito, Granada, Spain}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Green computing tries to push a series of best practices that, in general, reduce the amount of energy consumed to perform a given piece of work. There are no fixed rules for {\em greening} an algorithm implementation, which means that we need to create a methodology that, after profiling the energy spent by an algorithm implementation, comes up with specific rules that will optimize the amount of energy spent. In population based algorithms, the exploration/exploitation balance is one of the most critical aspects. The algorithm we will be working with in this paper called Brave New Algorithm was designed with the main objective of keeping that balance in an optimal way through the stratification of the population. In this paper we will analyze how this balance affects the energy consumption of the algorithm.


\keywords{Green computing  \and Energy profiling \and Metaheuristics.}
\end{abstract}
%
%
%
\section{Introduction}


Goldberg in his "Zen and the art of genetic algorithms" \cite{goldberg1989zen} included as a {\em koan} "Let nature be your guide". This has spawned all kind of population-based metaheuristics that use (and possibly abuse \cite{soni2021critical}) metaphors to inspire all kind of optimization algorithms, in most cases population-based.

All these algorithms have characteristics in common. Generally are population-based, that is, they work with a population of individuals that are (possibly encoded) solutions to a problem. There possible solutions are evaluated via a so-called {\em fitness} or {\em objective} function, according to which population is modified so that it is increasingly more likely to find solutions that are closer to the objective. What the algorithms do is explore the search space, using a combination of techniques:\begin{itemize}
\item {\em Exploration} techniques, which sample the search space looking for new solutions, generally via variation of the existing solutions.
\item {\em Exploitation} techniques, which try to improve existing solutions by first selecting those that are closer to the objective, and then combining them to create new solutions, thus {\em exploiting} the search directions already present in the population.
\end{itemize}

The algorithms and their specific implementations need to keep exploration in check, so that only {\em promising} parts of the search space are checked, that is, only solutions that have a certain probability of being better are generated; leaning on exploration will eventually lead to an algorithm that is even worse than exhaustive search, since probabilistically some solutions can be generated several times. Exploitation will, on the other hand, focus on some specific areas of the search space, those that already have {\em good} solutions, potentially falling into local minima and abandoning other areas that could hold much better solutions. This is why there needs to be a balanced approach to using both techniques, as most algorithms already do.

The performance of these algorithms is generally measured using quantities such as the number of fitness evaluations needed to reach a certain level of quality or the time needed to reach it. However, from the point of view of modern engineering energy needed by an algorithm implementation is as important, or even more, since an efficient implementation of an algorithm needs to be energy-efficient, or at least consume the least amount of energy possible.

This will be our main intention in this paper. We will work with an algorithm proposed in \cite{merelo2022brave} called Brave New Algorithm, which, instead of being biologically inspired directly, gets its main population structures from the dystopic novel Brave New World by Aldous Huxley \cite{huxley2022brave}; hence the name. This algorithm was designed with the main objective of keeping a healthy exploration/exploitation balance, which besides can be easily tuned through algorithm parameters. This makes it a good candidate for the analysis we will perform on this paper.

As secondary objective, we will be exploring the possibilities of implementation of metaheuristics in the language called Julia \cite{perkel2019julia}, a just-in-time compiled language that has an interesting way of dealing with allocation of memory; we will need to find out how, in this specific case, working with memory will affect consumption and how these mechanisms can be leveraged to optimize energy consumption.

The rest of the paper is organized as follows: next Section describes the state of the art in exploration of energy consumption of evolutionary algorithms and other metaheuristics. Section \ref{sec:bna} describes the algorithm and how it has been parametrized for the purpose of this paper. Since there is no fixed methodology for measuring energy consumption, we will show how it was done for this experiment and the tradeoffs it implies in Section \ref{sec:methodology}. Results will be presented in \ref{sec:results}, and eventually discussed in the last Section \ref{sec:discussion}.

\section{State of the art}

As a secondary objective in this paper, we have wanted to deal with implementation issues in a non-mainstream language; as a matter of fact, there are several implementations of evolutionary algorithms in Julia, such as \cite{sanchez2023evolp}. Before that, we had used in \cite{merelo2016comparison} Julia as a baseline for comparing performance in basic evolutionary algorithms; as a matter of fact, we used Julia as a baseline, with its performance being more or less the median of all languages tested. Some other languages like Java or C++ might be several orders of magnitude faster. However, performance does not translate directly to energy consumption, and besides, back then Julia was still in its early versions vs. the more mature implementation that is now in production.

Moreover, our main interest is optimizing energy consumption. This can be done at many different levels, from the lowest, hardware level, to the implementation level, up to the algorithmic level, clearly at this level the range of possible optimizations will be relatively small, but still it gives leverage to scientists that cannot change any other level to make their algorithm implementation {\em greener}. Some researchers have applied it to machine learning algorithms \cite{gutierrez2022analysing}; there are not so many cases where it has been applied to metaheuristics such as the one used in this paper. One of these was \cite{diaz2022population}, where they work mainly on the population, and conclude that an increasing population, which implies increased diversity and possibly better overall performance, has a complex relationship with energy consumption that might not be a direct one. If we just equate energy consumption with time needed to reach a solution, finding a parametrization that finds solution faster will always be a way of reducing it.

In this paper we will be focusing on such a parametrization mainly, applied to the Brave New Algorithm, which is essentially an evolutionary algorithm with population reproduction restrictions. We will describe this algorithm next.

\section{A Brave New Algorithm}\label{sec:bna}

\section{Results}\label{sec:results}
<<walcom.results, echo=FALSE, warnings=FALSE, message=F, fig.height=5, fig.cap="Energy as a function of time for all experiments; point size is related to the max number of generations">>=
walcom_results <- read.csv("data/sphere-walcom-bna-21-Sep-10-40-09.csv")
library(dplyr)
library(ggplot2)
library(tidyr)

walcom_results$log_diff <- log10(walcom_results$diff_fitness)
walcom_results$dimension <- as.factor(walcom_results$dimension)
walcom_results$population_size <- as.factor(walcom_results$population_size)
walcom_results$point_size <- ifelse(walcom_results$max_gens==10,2,4)
ggplot(walcom_results, aes(x=seconds,y=PKG,color=dimension,shape=population_size)) + geom_point(alpha=0.5,size=walcom_results$point_size) + theme_minimal() + labs(x="Time (s)", y="Energy (J)")

walcom_anova_pkg <- aov( lm ( PKG ~ dimension + population_size + max_gens + alpha, data=walcom_results ) )
walcom_anova_result_3 <- aov( lm ( log_diff ~ population_size + max_gens + alpha, data=walcom_results[ walcom_results$dimension == 3,] ) )
walcom_anova_result_5 <- aov( lm ( log_diff ~ population_size + max_gens + alpha, data=walcom_results[ walcom_results$dimension == 5,] ) )
@


The results of the experiments have been summarized in Figure \ref{fig:walcom.results}, which plots energy consumption in Joules vs time spent, representing problem dimension with different shapes and population size with different colors. It is interesting to note that problem dimension is not really a factor in energy consumption, since the algorithm runs until the maximum number of generations without change is reached; when the problem dimension is higher, it simply stops in a worse solution. Most points, however, independently of the problem dimension or other factors, are in the 400-450 Joules and 8-8.5 seconds range. At the end of the day, this means that any optimization we might be able to make in this specific problem will, at most, be in the 10\% range in the average case. This is why in most cases optimization will should focus on the worst case scenario, as we will see later.

This is an usual result, but we are more interested in checking what is the dependence of energy consumed on the different parameters of the algorithm. In order to check this, we have performed an ANOVA analysis of the results, which yields that the variation is significant below the 0.05 threshold for 3 of the 4 components: all but the maximum number of generations, which is used as a stopping criterion. Increasing the number of generations that the algorithm will run without finding a better solution seems to be mostly {\em free}, giving the algorithm designer a certain amount of freedom when choosing this parameter. This price might not be extended to other sizes or problems, so as is usual in the energy profiling of any metaheuristic, it is advisable to perform a specific analysis for each problem and size. Performing an analysis on the fitness obtained, the results are similar: there is no significant dependence on the result achieved.

<<walcom.models, echo=FALSE, message=F, warning=F>>=
library(kableExtra)
evaluations_model <- glm(evaluations ~ dimension + population_size + max_gens + alpha, data=walcom_results)

walcom_results %>% group_by(dimension,population_size,max_gens,alpha) %>%
  summarise(mean_energy=mean(PKG), sd_energy=sd(PKG),
            mean_time=mean(seconds), sd_time=sd(seconds),
            mean_evaluations=mean(evaluations), sd_evaluations=sd(evaluations),
            mean_generations=mean(generations), sd_gens=sd(generations),
            mean_diff_fitness=mean(log_diff), sd_diff_fitness=sd(log_diff)) -> summary_results

summary_results$energy <- paste0(round(summary_results$mean_energy,2), " (", round(summary_results$sd_energy,2), ")")
summary_results$time <- paste0(round(summary_results$mean_time,2), " (", round(summary_results$sd_time,2), ")")
summary_results$evaluations <- paste0(round(summary_results$mean_evaluations,2), " (", round(summary_results$sd_evaluations,2), ")")
summary_results$generations <- paste0(round(summary_results$mean_generations,2), " (", round(summary_results$sd_gens,2), ")")
summary_results$diff_fitness <- paste0(round(summary_results$mean_diff_fitness,2), " (", round(summary_results$sd_diff_fitness,2), ")")

summary_results <- summary_results[,c("dimension","population_size","max_gens","alpha","energy","time","evaluations","generations","diff_fitness")]
kable(summary_results, digits=2, booktabs=T, col.names = c("D","Pop. size", "Gens.", "Alpha %", "Energy", "Time (s)", "Evals", "Gens.", "log(diff. target)"),  caption="Summary of results") %>% kable_styling(full_width=F)
@

The whole table of results is shown in Table \ref{tab:walcom.models}, showing the average and standard deviation for the measures obtained for every combination of values: the average energy consumed in Joules, time in seconds, number of evaluations until completion, how many generations are needed, and finally the logaritm of the difference between the fitness objective and the best fitness found.

<<walcom.dimensions, echo=FALSE, message=F, warning=F, fig.height=4, fig.cap="Box plots of energy spent for the different dimensions">>=
ggplot(walcom_results, aes(x=dimension, y=PKG, fill=dimension)) + geom_boxplot( notch=T) + theme_minimal() + labs(x="Dimension", y="Energy (J)", title="Energy distribution by dimension") + theme(legend.position="none") + scale_y_log10()
@

Since the main objective of this paper was not to optimize the performance of the algorithm, we clearly observe that the fitness results for the Sphere function and the problem with 5 dimensions are not really competitive. This might explain why, in general, all dependent results are inferior: there are less evaluations, it takes less time on average and it stops sooner. We can obviously reverse this result: you normally need more energy to find better solutions. Please check Figure \ref{fig:walcom.dimensions} for a boxplot that compares energy needed for 3 and 5 dimensions, showing how the median is higher for the smaller problem, but again it is mainly because they explore better the solution space, finding better solutions. In this paper we were more interested in the exploration/exploitation balance for specific problem sizes, so we will analyze this in more detail for every problem size separately.

<<walcom.violin, echo=FALSE, message=F, warning=F, fig.height=3, fig.show='hold', fig.cap="Violin plots for the different dimensions">>=

for (dim in c(3,5)) {
  dim_results <- walcom_results[ walcom_results$dimension == dim,]
  p1 <- ggplot(dim_results, aes(x=population_size, y=PKG, fill=population_size)) + geom_violin() + theme_minimal() + labs(x="Population size", y="Energy (J)", title=paste("Energy distribution for dimension", dim)) + theme(legend.position="none")
  print(p1)
  dim_results$alpha <- as.factor(dim_results$alpha)
  p2 <- ggplot(dim_results, aes(x=alpha, y=PKG, fill=alpha)) + geom_violin() + theme_minimal() + labs(x="Alpha", y="Energy(J)", title=paste("Energy distribution for dimension", dim)) + theme(legend.position="none")
  print(p2)
}

@

<<walcom.energy, echo=FALSE, message=F, warning=F, fig.height=5, fig.cap="Result as a function of energy spent for all experiments. Point size is related to the alpha caste percentage. Lower is better in the y axis.">>=
walcom_results$point_size <- ifelse(walcom_results$alpha==10,2,4)
ggplot(walcom_results, aes(x=PKG,y=log_diff,color=dimension,shape=population_size)) + geom_point(alpha=0.5,size=walcom_results$point_size) + theme_minimal() + labs(x="Energy (J)", y="log(diff. target)")+
  scale_x_log10()
@

Finally, Figure \ref{fig:walcom.energy} tries to summarize the results, plotting the quality of the solution found (lower is better) as a function of energy spent in Joules. It again shows clearly that the problem with dimension 5 arrives to worse solutions, grouped in the upper part of the graph. If we look at these, we see that the energy spent to arrive more or less at the same solutions is higher when the population is higher. This points to a low diversity in the population, and a bad exploration/exploitation balance. From the point of view of energy spent, increasing the exploration capability by increasing the percentage of population in the $\alpha$ and $\beta$ castes might increase the energy spent, and in some cases obtain better solutions, leading to the best case of all experiments. However, the most important takeaway here is that there does not seem to be a correlation between energy spent and better solutions, and that tweaking the exploration/exploitation balance will not affect energy spent, having possibly a positive effect on the solutions. At any rate, solutions are suboptimal so this needs to be explored further.

This trend is probably more visible for problem dimension = 3, in the lower part of the chart. The best solutions use $\alpha = 25\%$ (with, correspondingly, $\beta = 50\%$), tipping the balance slightly more towards exploitation. But the outstanding part is that better solutions are obtained without implying bigger energy expenses, on the contrary, energy spent is around 400 Joules with few outliers.

This might be, in part, related to the implementation in the language Julia. Julia is extremely fast, which implies that the application of different operators has a negligible difference. The actual impact on the energy consumed must have a different origin.

<<walcom.evals, echo=FALSE, message=F , warning= F,  error=F, fig.height=5, fig.cap="Energy consumption as a function of evaluations for all experiments. Point size is related to the alpha caste percentage. Lower is better in the y axis.">>=
summary_pkg <- summary(lm( PKG ~ evaluations * generations, data=walcom_results))
ggplot(walcom_results, aes(x=evaluations,y=PKG,color=generations)) + geom_point(alpha=0.5) + theme_minimal() + labs(x="Evaluations", y="Energy (J)")
@

In Figure \ref{fig:walcom.evals} we plot the energy consumed by PKG vs. the number of evaluations, with the color related to the number of generations needed. A linear model shows significant dependence of energy consumed with the number of evaluations, the number of generations and also the interaction between the two variables. This dependence goes in an unexpected direction: energy spent decreases with the number of evaluations, but increases with the number of generations; in the chart, we can see that as we go to higher energy consumption values, the points are lighter, that is, the algorithm has run for more generations. This would encourage us to create populations that are as small as possible, and cramming as many evaluations as possible into every one of them, putting more elements of the population into the $\epsilon$ caste, which is the one that performs local search.


\section{Conclusion and discussion}
\label{sec:discussion}


A future line of work will take into account that different parametrizations are needed for different problems and sizes, so a more fair comparison of how resource consumption increases with size will need to perform some kind of parameter optimization for every size and problem, since at the end of the day what a scientist needs is to minimize the energy consumption for a solution quality level.


\section*{Acknowledgements}
This work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y Competitividad (Spanish Ministry of Competitivity and Economy) under project PID2023-147409NB-C21.

\section*{References}
\bibliographystyle{splncs04}
\bibliography{ours,ga-energy,GAs,julia}


\end{document}
